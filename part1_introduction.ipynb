{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cf2f9f",
   "metadata": {},
   "source": [
    "### Part 1: Introduction to Classification & Evaluation (`part1_introduction.ipynb`)\n",
    "\n",
    "1. **Load Data:** Load the `synthetic_health_data.csv` file into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48372cda",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71257fc1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(file_path): # load data using pandas\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found: '{file_path}'\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc386df",
   "metadata": {},
   "source": [
    "2. **Prepare Data:** Select relevant features and the target. Split the data into training and testing sets. Handle any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058336e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data_part1(df, test_size = 0.2, random_state = 42): # use template and select features, split into train/test, handle missing data\n",
    "    \n",
    "    features = ['age', 'systolic_bp', 'diastolic_bp', 'glucose_level', 'bmi']\n",
    "    target = 'disease_outcome'\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    imputer = SimpleImputer(strategy = 'mean')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size = test_size, random_state = random_state, stratify = y)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24941a71",
   "metadata": {},
   "source": [
    "\n",
    "3. **Train Model:** Train a Logistic Regression model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7337c05",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = load_data('data/synthetic_health_data.csv')\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b92b3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data_part1(df)\n",
    "\n",
    "print(\"Train set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c174e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train):\n",
    "    \n",
    "    model = LogisticRegression(max_iter = 500)  \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170e8c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = train_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d201726",
   "metadata": {},
   "source": [
    "\n",
    "4. **Evaluate Model:** Calculate accuracy, precision, recall, F1 score, AUC, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643216ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_evaluation_metrics(model, X_test, y_test):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])  # setting up the probability of a positive result to compare AUC score\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99629630",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "metrics = calculate_evaluation_metrics(model, X_test, y_test)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a4b17",
   "metadata": {},
   "source": [
    "\n",
    "5. **Save Results:** Save the metrics to `results/results_part1.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f831a57",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_results(metrics, file_path = 'results/results_part1.txt'):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok = True) # make directory if no exist\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(\"Model Evaluation Metrics\\n\")\n",
    "        for metric, value in metrics.items():\n",
    "            if metric == 'confusion_matrix':\n",
    "                f.write(f\"{metric}:\\n{value}\\n\") # make the thing 2x2\n",
    "            else:\n",
    "                f.write(f\"{metric}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa572c29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "save_results(metrics, 'results/results_part1.txt')\n",
    "\n",
    "print(\"Results saved to 'results/results_part1.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff18014",
   "metadata": {},
   "source": [
    "\n",
    "6. **Interpret Results:** Implement a function `interpret_results(metrics)` that analyzes the model performance on imbalanced data. The function should return a dictionary with keys 'best_metric', 'worst_metric', and 'imbalance_impact_score' (a custom score from 0-1 indicating how much the class imbalance affected results). Additionally, write your manual interpretation of these results in a file called `RESULTS.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c055f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def interpret_results(metrics):\n",
    "    \n",
    "    # set the mini stuff\n",
    "    best_metric = None\n",
    "    worst_metric = None\n",
    "    imbalance_impact_score = 0\n",
    "    \n",
    "    # use the metrics set up from before\n",
    "    f1 = metrics['f1']\n",
    "    precision = metrics['precision']\n",
    "    recall = metrics['recall']\n",
    "    auc = metrics['auc']\n",
    "    \n",
    "    # compare the f1 and AUC to see which is higher \n",
    "    # f1 is looking at precision and recall\n",
    "    # AUC for identification of correct positives and negatives\n",
    "    best_metric_value = max(f1, auc)\n",
    "    if best_metric_value == f1:\n",
    "        best_metric = 'f1'\n",
    "    else:\n",
    "        best_metric = 'auc'\n",
    "    \n",
    "    # assuming imbalanced set, accuracy doesnt really tell us a lot of info from the data other than how much is right and wrong\n",
    "    accuracy = metrics['accuracy']    \n",
    "    worst_metric = 'accuracy'\n",
    "    \n",
    "    # if recall and precision are the same, then there's likely no imbalance\n",
    "    # if they're different, then it is possible there is some bias or imbalance\n",
    "    imbalance_impact_score = abs(recall - precision)\n",
    "    \n",
    "    # set in dictionary\n",
    "    return {\n",
    "        'best_metric': best_metric,\n",
    "        'worst_metric': worst_metric,\n",
    "        'imbalance_impact_score': imbalance_impact_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40df8ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "interpretation = interpret_results(metrics)\n",
    "\n",
    "# Print the interpretation\n",
    "print(\"Best Metric:\", interpretation['best_metric'])\n",
    "print(\"Worst Metric:\", interpretation['worst_metric'])\n",
    "print(\"Imbalance Impact Score:\", interpretation['imbalance_impact_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4cc8a",
   "metadata": {},
   "source": [
    "### PUT IT ALL TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa105081",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load data\n",
    "    data_file = 'data/synthetic_health_data.csv'\n",
    "    df = load_data(data_file)\n",
    "    \n",
    "    # 2. Prepare data\n",
    "    X_train, X_test, y_train, y_test = prepare_data_part1(df)\n",
    "    \n",
    "    # 3. Train model\n",
    "    model = train_logistic_regression(X_train, y_train)\n",
    "    \n",
    "    # 4. Evaluate model\n",
    "    metrics = calculate_evaluation_metrics(model, X_test, y_test)\n",
    "    \n",
    "    # 5. Print metrics\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'confusion_matrix':\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # 6. Save results\n",
    "    save_results(metrics, 'results/results_part1.txt')\n",
    "    \n",
    "    # 7. Interpret results\n",
    "    interpretation = interpret_results(metrics)\n",
    "    print(\"\\nResults Interpretation:\")\n",
    "    for key, value in interpretation.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
